{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Petfood Review Classification with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = ### bucket name  \n",
    "PROJECT = ### project name    \n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting BERT_Model/trainer/BERT_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile BERT_Model/trainer/BERT_model.py\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "from tensorflow.python.platform import tf_logging\n",
    "import sys as _sys\n",
    "import datetime\n",
    "\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "\n",
    "    \n",
    "CLASSES = {'health': 0, 'quality': 1, 'product':2}  # label-to-int mapping\n",
    "DATA_COLUMN = 'text'\n",
    "LABEL_COLUMN = 'label'\n",
    "labels = [0, 1, 2]\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "MAX_SEQ_LENGTH = 128  # Sentences will be truncated/padded to this length\n",
    "SAVE_CHECKPOINTS_STEPS = 500\n",
    "SAVE_SUMMARY_STEPS = 100\n",
    "\n",
    "def load_review_data(train_data_path, eval_data_path):\n",
    "    \n",
    "    \"\"\"\n",
    "        Parses raw tsv containing body of reviews and returns train and eval dataframes \n",
    "        containing body of reviews and their labels\n",
    "            - train_data_path: string, path to tsv containing training data.\n",
    "                can be a local path or, S3 url (s3://...), or GCS url (gs://...)\n",
    "            - eval_data_path: string, path to tsv containing eval data.\n",
    "                can be a local path or a S3 url (s3://...), or GCS url (gs://...)\n",
    "          Returns:\n",
    "              train and eval dataframes containing integer labels and body of reviews\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parse CSV using pandas\n",
    "    column_names = ('label', 'text')\n",
    "    df_train = pd.read_csv(train_data_path, names=column_names, sep='\\t')\n",
    "    df_eval = pd.read_csv(eval_data_path, names=column_names, sep='\\t')\n",
    "    \n",
    "    # Convert labels from text to int\n",
    "    df_train['label'] = df_train['label'].map(CLASSES)\n",
    "    df_eval['label'] = df_eval['label'].map(CLASSES)\n",
    "    \n",
    "    return df_train, df_eval\n",
    "\n",
    "\n",
    "\n",
    "def bert_preprocess(df_train, df_eval):\n",
    "    \n",
    "    \"\"\"\n",
    "        Transforms data into a format understandable by BERT, by creating InputExample's \n",
    "        using the constructor provided in the BERT library.\n",
    "            - guid: globally unique ID for bookkeeping. Not used here\n",
    "            - text_a: the text we want to classify, i.e. DATA_COLUMN\n",
    "            - text_b: used when training a model to understand the relationship between sentences. \n",
    "                Doesn't apply here, so it is left blank.\n",
    "            - label: the label or class of the review (0, 1, or 2)\n",
    "        Returns:\n",
    "            train_InputExamples and eval_InputExamples\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    # Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "    train_InputExamples = df_train.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), \n",
    "                                                                     axis = 1)\n",
    "\n",
    "    eval_InputExamples = df_eval.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), \n",
    "                                                                   axis = 1)\n",
    "    \n",
    "    return train_InputExamples, eval_InputExamples\n",
    "\n",
    "\n",
    "\n",
    "def bert_tokenizer():\n",
    "    \n",
    "    \"\"\"\n",
    "        Gets the vocab file and casing info from the BERT Hub module, and\n",
    "        returns BERT tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                                  tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "    return bert.tokenization.FullTokenizer(vocab_file=vocab_file, \n",
    "                                           do_lower_case=do_lower_case)\n",
    "\n",
    "\n",
    "def get_features(train_InputExamples, eval_InputExamples, tokenizer):\n",
    "    \n",
    "    \"\"\"\n",
    "        Converts InputExamples to InputFeatures understandable by BERT, using \n",
    "        \"convert_examples_to_features\" provided in BERT library, and BERT tokenizer\n",
    "        \n",
    "        Returns:\n",
    "            train_features and eval_features\n",
    "    \"\"\"\n",
    "    \n",
    "    train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, \n",
    "                                                                      labels, \n",
    "                                                                      MAX_SEQ_LENGTH, \n",
    "                                                                      tokenizer)\n",
    "    eval_features = bert.run_classifier.convert_examples_to_features(eval_InputExamples, \n",
    "                                                                     labels, \n",
    "                                                                     MAX_SEQ_LENGTH, \n",
    "                                                                     tokenizer)\n",
    "    return train_features, eval_features\n",
    "\n",
    "\n",
    "\n",
    "def BERT_model(input_ids, input_mask, segment_ids, label_ids, num_labels,\n",
    "               dropout_rate,predict):\n",
    "    \"\"\"\n",
    "        Creates a classification model, by loading the BERT tf-hub module to extract the \n",
    "        computation graph, and fine-tuning BERT by createing a trainable layer to adapt BERT \n",
    "        to the specific classification task.\n",
    "        \n",
    "        Returns:\n",
    "            loss, predicted labels, and log softmax probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Define BERT module from BERT_MODEL_HUB, inputs and outputs\n",
    "    module = hub.Module(BERT_MODEL_HUB,\n",
    "                        trainable=True)\n",
    "    \n",
    "    inputs = dict(input_ids=input_ids,\n",
    "                  input_mask=input_mask,\n",
    "                  segment_ids=segment_ids)\n",
    "    \n",
    "    outputs = module(inputs=inputs,\n",
    "                     signature=\"tokens\",\n",
    "                     as_dict=True)\n",
    "    \n",
    "    ### Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "    output_layer = outputs[\"pooled_output\"]\n",
    "\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    ### Create Trainable BERT layer\n",
    "    output_weights = tf.compat.v1.get_variable(\"output_weights\", \n",
    "                                     [num_labels, hidden_size],\n",
    "                                     initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    output_bias = tf.compat.v1.get_variable(\"output_bias\", \n",
    "                                  [num_labels], \n",
    "                                  initializer=tf.zeros_initializer())\n",
    "\n",
    "    with tf.compat.v1.variable_scope(\"loss\"):\n",
    "        output_layer = tf.nn.dropout(output_layer, rate=dropout_rate)   \n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "        # one-hot encoding labels\n",
    "        one_hot_labels = tf.one_hot(label_ids, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "        \n",
    "        # Return predicted labels and probabilities in prediction mode\n",
    "        if predict:\n",
    "            return (predicted_labels, log_probs)\n",
    "\n",
    "        # Compute loss in Train and Eval modes\n",
    "        instance_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(instance_loss)\n",
    "        return (loss, predicted_labels, log_probs)\n",
    "    \n",
    "\n",
    "def metric_fn(label_ids, predicted_labels):\n",
    "    \"\"\"\n",
    "        Adds evaluation metrics to EstimatorSpec when mode == tf.estimator.ModeKeys.EVAL\n",
    "        \n",
    "        Returns:\n",
    "            evaluation accuracy, precision, and recall\n",
    "    \"\"\"\n",
    "    accuracy = tf.compat.v1.metrics.accuracy(label_ids, predicted_labels)\n",
    "    recall = tf.compat.v1.metrics.recall(label_ids,predicted_labels)\n",
    "    precision = tf.compat.v1.metrics.precision(label_ids,predicted_labels) \n",
    "                \n",
    "    return {\"eval_accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall}\n",
    "\n",
    "\n",
    "def model_fn_builder(num_labels, learning_rate, dropout_rate,num_train_steps,num_warmup_steps):\n",
    "    \"\"\"Returns 'model_fn' used in the Estimator\"\"\"\n",
    "    \n",
    "    def model_fn(features, labels, mode, params):  \n",
    "\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "\n",
    "        predict = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "        ### TRAIN and EVAL\n",
    "        if not predict:\n",
    "            (loss, predicted_labels, log_probs) = BERT_model(input_ids, \n",
    "                                                             input_mask, \n",
    "                                                             segment_ids, \n",
    "                                                             label_ids, \n",
    "                                                             num_labels,\n",
    "                                                             dropout_rate,\n",
    "                                                             predict)\n",
    "            \n",
    "            train_optimizer = bert.optimization.create_optimizer(loss, \n",
    "                                                                 learning_rate, \n",
    "                                                                 num_train_steps, \n",
    "                                                                 num_warmup_steps, \n",
    "                                                                 use_tpu=False)\n",
    "\n",
    "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                                  loss=loss,\n",
    "                                                  train_op=train_optimizer)\n",
    "            \n",
    "            else:\n",
    "                eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "                return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                                  loss=loss,\n",
    "                                                  eval_metric_ops=eval_metrics)\n",
    "        ### Predict\n",
    "        else:\n",
    "            (predicted_labels, log_probs) = BERT_model(input_ids, \n",
    "                                                       input_mask, \n",
    "                                                       segment_ids, \n",
    "                                                       label_ids, \n",
    "                                                       num_labels,\n",
    "                                                       dropout_rate,\n",
    "                                                       predict)\n",
    "\n",
    "            predictions = {'probabilities': log_probs,\n",
    "                           'labels': predicted_labels}\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    # Return the model function\n",
    "    return model_fn\n",
    "\n",
    "\n",
    "def serving_input_fn():\n",
    "    \"\"\"\n",
    "        Defines the features to be passed to the model during inference\n",
    "        Can pass in string text directly. \n",
    "\n",
    "        Returns: \n",
    "            tf.estimator.export.build_raw_serving_input_receiver_fn\n",
    "    \"\"\"\n",
    "    label_ids = tf.placeholder(tf.int32, [None], name='label_ids')\n",
    "    input_ids = tf.placeholder(tf.int32, [None, MAX_SEQ_LENGTH], name='input_ids')\n",
    "    input_mask = tf.placeholder(tf.int32, [None, MAX_SEQ_LENGTH], name='input_mask')\n",
    "    segment_ids = tf.placeholder(tf.int32, [None, MAX_SEQ_LENGTH], name='segment_ids')\n",
    "    serve_fn = tf.estimator.export.build_raw_serving_input_receiver_fn({\n",
    "        'label_ids': label_ids,\n",
    "        'input_ids': input_ids,\n",
    "        'input_mask': input_mask,\n",
    "        'segment_ids': segment_ids,\n",
    "    })()\n",
    "\n",
    "    return serve_fn\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate(model_dir, hparams):\n",
    "    \"\"\"\n",
    "        Main orchestrator. Responsible for calling all other functions in BERT_model.py\n",
    "\n",
    "            model_dir: string, file path where training files will be written\n",
    "            hparams: dict, command line parameters passed from task.py\n",
    "    \n",
    "        Returns: \n",
    "            Starts training and evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    df_train, df_eval = load_review_data(hparams['train'], hparams['eval'])\n",
    "    train_InputExamples, eval_InputExamples = bert_preprocess(df_train, df_eval)\n",
    "    tokenizer = bert_tokenizer()\n",
    "    train_features, eval_features = get_features(train_InputExamples, eval_InputExamples, tokenizer)\n",
    "    \n",
    "    ### Compute number of train and warmup steps from batch size\n",
    "    num_train_steps = int(len(train_features) / hparams['batch_size'] * hparams['num_train_epochs'])\n",
    "    num_warmup_steps = int(num_train_steps * hparams['warmup_proportion'])\n",
    "\n",
    "    ### Specify outpit directory and number of checkpoint steps to save\n",
    "    run_config = tf.estimator.RunConfig(model_dir=model_dir,\n",
    "                                        save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "                                        save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
    "    \n",
    "    model_fn = model_fn_builder(num_labels=len(labels),\n",
    "                                learning_rate=hparams['learning_rate'],\n",
    "                                dropout_rate = hparams['dropout_rate'],\n",
    "                                num_train_steps=num_train_steps,\n",
    "                                num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "    estimator = tf.estimator.Estimator(model_fn=model_fn,\n",
    "                                       config=run_config,\n",
    "                                       params={\"batch_size\": hparams['batch_size']})\n",
    "    \n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "    \n",
    "    ### Create input functions for training and evaluating. drop_remainder = True for using TPUs.\n",
    "    train_input_fn = bert.run_classifier.input_fn_builder(features=train_features,\n",
    "                                                          seq_length=MAX_SEQ_LENGTH,\n",
    "                                                          is_training=True,\n",
    "                                                          drop_remainder=False)\n",
    "    \n",
    "    eval_input_fn = run_classifier.input_fn_builder(features=eval_features,\n",
    "                                                    seq_length=MAX_SEQ_LENGTH,\n",
    "                                                    is_training=False,\n",
    "                                                    drop_remainder=False)\n",
    "    \n",
    "    train_spec = tf.estimator.TrainSpec(input_fn = train_input_fn,\n",
    "                                        max_steps=num_train_steps)\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn, \n",
    "                                      steps=None,\n",
    "                                      exporters=exporter,\n",
    "                                      start_delay_secs=10,\n",
    "                                      throttle_secs=10)\n",
    "    \n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting BERT_Model/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile BERT_Model/trainer/task.py\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "from . import BERT_model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "        \n",
    "    parser.add_argument(\n",
    "        '--model_dir',\n",
    "        type=str, \n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--train',\n",
    "        type=str, \n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval',\n",
    "        type=str, \n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num_train_epochs',\n",
    "        default = 1.0,\n",
    "        type= float\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--batch_size',\n",
    "        default=32,\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        default=0.001,\n",
    "        type=float\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--dropout_rate',\n",
    "        help='percentage of input to drop at Dropout layers',\n",
    "        default=.2,\n",
    "        type=float\n",
    "    )   \n",
    "    parser.add_argument(\n",
    "        '--warmup_proportion',\n",
    "        help = 'Warmup is a period of time when learning rate is small and gradually increases--usually helps training',\n",
    "        default=0.1,\n",
    "        type=float\n",
    "    )\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    hparams = args.__dict__\n",
    "    model_dir = hparams.pop('model_dir')\n",
    "    \n",
    "    BERT_model.train_and_evaluate(model_dir, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing BERT_Model/trainer/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile BERT_Model/trainer/__init__.py\n",
    "\n",
    "### init file ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on Google AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://bert-review/trained_model us-central1 reviews_200621_025757\n",
      "jobId: reviews_200621_025757\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://bert-review/trained_model/checkpoint#1592707768249995...\n",
      "Removing gs://bert-review/trained_model/eval/#1592707848719089...\n",
      "Removing gs://bert-review/trained_model/eval/events.out.tfevents.1592707850.gke-cml-0621-021210--n1-standard-4-f5-e8b89c58-1slr#1592707854647650...\n",
      "Removing gs://bert-review/trained_model/events.out.tfevents.1592707192.gke-cml-0621-021210--n1-standard-4-f5-e8b89c58-zjsf#1592707797726339...\n",
      "Removing gs://bert-review/trained_model/events.out.tfevents.1592707200.gke-cml-0621-021210--n1-standard-4-f5-e8b89c58-1slr#1592707772439602...\n",
      "Removing gs://bert-review/trained_model/export/#1592707855805937...\n",
      "Removing gs://bert-review/trained_model/export/exporter/#1592707856187191...\n",
      "Removing gs://bert-review/trained_model/export/exporter/temp-1592707854/#1592707856638097...\n",
      "Removing gs://bert-review/trained_model/model.ckpt-504.data-00001-of-00003#1592707766216933...\n",
      "Removing gs://bert-review/trained_model/model.ckpt-504.data-00002-of-00003#1592707765244005...\n",
      "Removing gs://bert-review/trained_model/graph.pbtxt#1592707221996785...\n",
      "Removing gs://bert-review/trained_model/model.ckpt-0.data-00001-of-00003#1592707233531978...\n",
      "Removing gs://bert-review/trained_model/model.ckpt-0.data-00002-of-00003#1592707232654033...\n",
      "Removing gs://bert-review/trained_model/model.ckpt-0.data-00000-of-00003#1592707233034670...\n",
      "Removing gs://bert-review/trained_model/model.ckpt-0.index#1592707234027238...\n",
      "Removing gs://bert-review/trained_model/model.ckpt-504.index#1592707766726141...\n",
      "Removing gs://bert-review/trained_model/model.ckpt-0.meta#1592707238181387...\n",
      "Removing gs://bert-review/trained_model/model.ckpt-504.meta#1592707770947430...\n",
      "Removing gs://bert-review/trained_model/model.ckpt-504.data-00000-of-00003#1592707765723759...\n",
      "Removing gs://bert-review/trained_model/packages/a35a90fec84517ab2f824069ef7eea9feea1c1b99f13bbd12530267631f8d3d1/BERT_Model-0.1.tar.gz#1592707114115534...\n",
      "/ [20/20 objects] 100% Done                                                     \n",
      "Operation completed over 20 objects.                                             \n",
      "DEBUG: Running [gcloud.ai-platform.jobs.submit.training] with arguments: [--job-dir: \"<googlecloudsdk.api_lib.storage.storage_util.ObjectReference object at 0x7f8039d85d90>\", --master-accelerator: \"OrderedDict([(u'count', 4), (u'type', TypeValueValuesEnum(NVIDIA_TESLA_P100, 2))])\", --master-machine-type: \"n1-standard-4\", --module-name: \"trainer.task\", --package-path: \"/home/jupyter/BERT_Model/trainer\", --parameter-server-count: \"3\", --parameter-server-machine-type: \"n1-standard-4\", --region: \"us-central1\", --runtime-version: \"1.15\", --scale-tier: \"custom\", --verbosity: \"debug\", --worker-accelerator: \"OrderedDict([(u'count', 4), (u'type', TypeValueValuesEnum(NVIDIA_TESLA_P100, 2))])\", --worker-count: \"5\", --worker-machine-type: \"n1-standard-4\", JOB: \"reviews_200621_025757\"]\n",
      "DEBUG: Looking for setup.py file at [/home/jupyter/BERT_Model/setup.py]\n",
      "INFO: Using existing setup.py file at [/home/jupyter/BERT_Model/setup.py]\n",
      "DEBUG: Executing command: ['/usr/bin/python2', u'-S', u'/home/jupyter/BERT_Model/setup.py', u'egg_info', u'--egg-base', '/tmp/tmpD9uNut', u'build', u'--build-base', '/tmp/tmpD9uNut', u'--build-temp', '/tmp/tmpD9uNut', u'sdist', u'--dist-dir', u'/tmp/tmpi2TKlu/output']\n",
      "DEBUG: Executing command: ['/usr/bin/python2', u'/home/jupyter/BERT_Model/setup.py', u'egg_info', u'--egg-base', '/tmp/tmpD9uNut', u'build', u'--build-base', '/tmp/tmpD9uNut', u'--build-temp', '/tmp/tmpD9uNut', u'sdist', u'--dist-dir', u'/tmp/tmpi2TKlu/output']\n",
      "DEBUG: Python packaging resulted in [/tmp/tmpi2TKlu/output/BERT_Model-0.1.tar.gz]\n",
      "INFO: Uploading [/tmp/tmpi2TKlu/output/BERT_Model-0.1.tar.gz] to [bert-review/trained_model/packages/fe7c62f0927c70f68ba049531764edb77d3b8b58fb502ede19384ae142e1cd14/BERT_Model-0.1.tar.gz]\n",
      "DEBUG: Using [u'gs://bert-review/trained_model/packages/fe7c62f0927c70f68ba049531764edb77d3b8b58fb502ede19384ae142e1cd14/BERT_Model-0.1.tar.gz'] as trainer uris\n",
      "Job [reviews_200621_025757] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe reviews_200621_025757\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs reviews_200621_025757\n",
      "INFO: Display format: \"yaml(jobId,state,startTime.date(tz=LOCAL),endTime.date(tz=LOCAL))\"\n",
      "DEBUG: SDK update checks are disabled.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/trained_model\n",
    "JOBNAME=reviews_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud --verbosity=debug ai-platform jobs submit training $JOBNAME \\\n",
    " --region=$REGION \\\n",
    " --module-name=trainer.task \\\n",
    " --package-path=${PWD}/BERT_Model/trainer \\\n",
    " --job-dir=$OUTDIR \\\n",
    " --scale-tier=custom \\\n",
    " --master-machine-type=n1-standard-4 \\\n",
    " --master-accelerator count=4,type=nvidia-tesla-p100 \\\n",
    " --worker-count=5 \\\n",
    " --worker-machine-type=n1-standard-4 \\\n",
    " --worker-accelerator count=4,type=nvidia-tesla-p100 \\\n",
    " --parameter-server-count 3 \\\n",
    " --parameter-server-machine-type=n1-standard-4 \\\n",
    " --runtime-version=1.15 \\\n",
    " --\\\n",
    " --model_dir=$OUTDIR \\\n",
    " --train=gs://${BUCKET}/train.tsv \\\n",
    " --eval=gs://${BUCKET}/eval.tsv \\\n",
    " --batch-size=64 \\\n",
    " --num_train_epochs=10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gcloud ai-platform jobs stream-logs reviews_200621_025757"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "global_step = 4845: eval_accuracy = 0.43551716, loss = 1.080725, precision = 0.78562045, recall = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using endpoint [https://ml.googleapis.com/]\n",
      "Creating version (this might take a few minutes)......\n",
      "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%bash\n",
    "MODEL_NAME=\"BERT_model\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/trained_model/export/exporter/ | tail -1)\n",
    "#gcloud ai-platform versions delete ${MODEL_VERSION} --model ${MODEL_NAME} --quiet\n",
    "#gcloud ai-platform models delete ${MODEL_NAME}\n",
    "#gcloud ai-platform models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ai-platform versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version=1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf-gpu.1-15.m48",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m48"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
